{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Spark session configuration\n",
                "This cell sets Spark session settings to enable _Verti-Parquet_ and _Optimize on Write_. More details about _Verti-Parquet_ and _Optimize on Write_ in tutorial document."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T23:14:44.0639013Z\",\"session_start_time\":\"2023-04-14T23:14:44.2928543Z\",\"execution_start_time\":\"2023-04-14T23:14:55.3941283Z\",\"execution_finish_time\":\"2023-04-14T23:14:57.3983682Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"
            },
            "outputs": [],
            "source": [
                "# Copyright (c) Microsoft Corporation.\n",
                "# Licensed under the MIT License.\n",
                "\n",
                "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
                "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
                "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### Approach #1 - sale_by_date_city\n",
                "In this cell, you are creating three different Spark dataframes, each referencing an existing delta table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T23:14:44.0653167Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-04-14T23:14:57.8304026Z\",\"execution_finish_time\":\"2023-04-14T23:15:57.0058225Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "df_fact_sale = spark.read.table(\"Gold.FactSales\") \n",
                "df_dimension_date = spark.read.table(\"Gold.DimDate\")\n",
                "df_dimension_city = spark.read.table(\"Gold.DimCity\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "In this cell, you are joining these tables using the dataframes created earlier, doing group by to generate aggregation, renaming few of the columns and finally writing it as delta table in the _Tables_ section of the lakehouse."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T23:14:44.0704646Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-04-14T23:15:57.4087615Z\",\"execution_finish_time\":\"2023-04-14T23:16:16.4931219Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "sale_by_date_city = df_fact_sale.alias(\"sale\") \\\n",
                ".join(df_dimension_date.alias(\"date\"), df_fact_sale.InvoiceDateKey == df_dimension_date.Date, \"inner\") \\\n",
                ".join(df_dimension_city.alias(\"city\"), df_fact_sale.CityKey == df_dimension_city.CityKey, \"inner\") \\\n",
                ".select(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"city.City\", \"city.StateProvince\", \"city.SalesTerritory\", \"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\\\n",
                ".groupBy(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"city.City\", \"city.StateProvince\", \"city.SalesTerritory\")\\\n",
                ".sum(\"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\\\n",
                ".withColumnRenamed(\"sum(TotalExcludingTax)\", \"SumOfTotalExcludingTax\")\\\n",
                ".withColumnRenamed(\"sum(TaxAmount)\", \"SumOfTaxAmount\")\\\n",
                ".withColumnRenamed(\"sum(TotalIncludingTax)\", \"SumOfTotalIncludingTax\")\\\n",
                ".withColumnRenamed(\"sum(Profit)\", \"SumOfProfit\")\\\n",
                ".orderBy(\"date.Date\", \"city.StateProvince\", \"city.City\")\n",
                "\n",
                "sale_by_date_city.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/aggregate_sale_by_date_city\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### Approach #2 - sale_by_date_employee\n",
                "In this cell, you are creating a new Delta table by joining 3 tables, doing group by to generate aggregation, renaming few of the columns. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T23:14:44.0716871Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-04-14T23:16:16.8047041Z\",\"execution_finish_time\":\"2023-04-14T23:16:26.9405173Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
                "collapsed": false,
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "sparksql"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "CREATE OR REPLACE TABLE sale_by_date_employee\n",
                "USING DELTA\n",
                "AS\n",
                "SELECT\n",
                "\tDD.Date, DD.CalendarMonthLabel\n",
                "    , DD.Day, DD.ShortMonth Month, CalendarYear Year\n",
                "\t,DE.PreferredName, DE.Employee\n",
                "\t,SUM(FS.TotalExcludingTax) SumOfTotalExcludingTax\n",
                "\t,SUM(FS.TaxAmount) SumOfTaxAmount\n",
                "\t,SUM(FS.TotalIncludingTax) SumOfTotalIncludingTax\n",
                "\t,SUM(Profit) SumOfProfit \n",
                "FROM Gold.FactSales FS\n",
                "INNER JOIN Gold.DimDate DD ON FS.InvoiceDateKey = DD.Date\n",
                "INNER JOIN Gold.DimEmployee DE ON FS.SalespersonKey = DE.EmployeeKey\n",
                "GROUP BY DD.Date, DD.CalendarMonthLabel, DD.Day, DD.ShortMonth, DD.CalendarYear, DE.PreferredName, DE.Employee\n",
                "ORDER BY DD.Date ASC, DE.PreferredName ASC, DE.Employee ASC"
            ]
        }
    ],
    "metadata": {
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "display_name": "Synapse PySpark",
            "language": "Python",
            "name": "synapse_pyspark"
        },
        "language_info": {
            "name": "python"
        },
        "notebook_environment": {},
        "save_output": true,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {},
                "enableDebugMode": false
            }
        },
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        },
        "trident": {
            "lakehouse": {}
        },
        "widgets": {}
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
